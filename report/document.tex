\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}


% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[final]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{COMP4901I - BIIS - Assignment 4 Report}

\author{%
	Cheng Chi Fung \\
	\texttt{cfchengac@connect.ust.hk} \\
}

\begin{document}

\maketitle

\section{Data}

\subsection{Preprocessing}

In the data clearning part, we first turn all the unicode string into ASCII and lower case. Then, we expand the contradiction and remove all the characters except (A-Za-z0-9,!?;). Finally, the preprcessed data are tokenized by the white space. 

\subsection{Data Statistics}
The following are the data statistics.

\begin{table}[htb]
	\caption{Data Statistics}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Statistics & --  \\
		\midrule
		Number of sentence & 240234 \\
		Number of words & 6966786  \\
		Number of vocabs & 84624 \\
		Number of vocabs (After Limited) & 15000 \\
		Number of vocabs with minimum Frequency 3 & 38221 \\
		10 Most Frequent words & the, and, of, to, a, i, in, he ,was, that \\
		UNK Token Rate (Limited 15000 Vocabs) & 0.00312 \\
		\bottomrule
	\end{tabular}
\end{table}

\section{Results and Analysis}

\subsection{Hyperparameters Tuning Results}
The following are the hyperparameters tuning results.

\begin{table}[htb]
	\caption{Hyperparameter tuning results}
	\label{sample-table}
	\centering
	\begin{tabular}{lllllll}
		\toprule
		\cmidrule{1-4}
		Learning rate & Num of Hidden Layers & Hidden Dimension Size & Validation Perplexity\\
		\midrule
		0.0001 & 1 & 128 & 227.2763346960948 \\
		0.0001 & 1  & 256 & 211.13846383756055  \\
		0.0001 & 1  & 512 & 206.77620995328408  \\
		0.0001 & 2 & 512 & 201.65396298175335  \\
		0.001 & 2  & 512 & 535.7280983715222  \\
		\bottomrule
	\end{tabular}
\end{table}

Best Parameters obtained = (Learning rate: 0.0001, Number Hidden Layer: 2, Hidden Dimension Size: 512)

We use the model with the best Hyperparameters and obtain the following test perplexity.

\pagebreak

\begin{table}[htb]
	\caption{Test Results}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset & Best Test Perplexity\\
		\midrule
		Test Set & 0.63   \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Discuss of Parameters Tuning Reults}

\subsection{Train with Pretrained Word embeddings}
For Pretrained Word Embedding, we have tried to replace the original word embedding layer by the pretrained \textbf{word2Vector} with \textbf{Google News corpus} (3 billion running words) word vector model. (Google News Corpus: https://github.com/mmihaltz/word2vec-GoogleNews-vectors). And since the dimension of the embedding matrix is enormously big which cause some memory error during training, we have limited to only use ten thousands of vocabs. All above process can be easily done through by a python libarary named \textbf{gensim}. And the following are the results of using pretrained embedding. \textbf{(This part are implemented in the files with postfix \_embedding.py)}

We train the model with pretrained word embedding with the best hyperarameters and obtain the following test results.

\begin{table}[htb]
	\caption{Test Results with Pretrained word Embedding}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset & Best Test Perplexity\\
		\midrule
		Test Set & 0.63   \\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Language Generation}
The following are the short paragraph generated from our models using the folloing words as starting.

\paragraph{Start from "I"}

\paragraph{Start from "What"}

\paragraph{Start from "Anyway"}

\section{Bonus}

\subsection{Char Level RNN}
For the bonus part, we have implmented Char Level RNN with reference of the Char Level CNN. Samiliar as Char Level CNN, we have defined a list of characters which includes 26 English letters, 10 digits, 5 special characters ,a blank character, a start sentence and a end of sentence character. (\textbf{44 Characters in total}). 

In the later part, we convert the each character in the seqence into index and input into the RNN same as the word level RNN performing.  \textbf{(This bonus are implemented in the files with postfix \_char\_rnn.py)}

We train the char level rnn with the best hyperarameters and obtain the following test results.

\begin{table}[htb]
	\caption{Test Results of Char Level RNN}
	\label{sample-table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\cmidrule{1-2}
		Dataset & Best Test Perplexity\\
		\midrule
		Test Set & 3.762   \\
		\bottomrule
	\end{tabular}
\end{table}

\end{document}